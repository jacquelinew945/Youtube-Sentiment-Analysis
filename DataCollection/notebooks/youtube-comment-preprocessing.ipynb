{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d78c5ad-1bd0-4f44-a0a2-7fde14ae63b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_name = \"youtube-news-comments\"\n",
    "mount_point = f\"/mnt/{mount_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb4a72e-5b1b-47fd-b030-fbe0053487de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_items = dbutils.fs.ls(mount_point+\"/youtube-comments/\")\n",
    "json_files = [item.path for item in all_items if item.path.endswith(\".json\")]\n",
    "new_file = sorted(json_files, reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1769186b-92ee-422d-b084-f78189e59300",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- author: string (nullable = true)\n |-- comment_id: string (nullable = true)\n |-- like_count: long (nullable = true)\n |-- published_at: string (nullable = true)\n |-- text: string (nullable = true)\n |-- video_id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(new_file)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ae0cac-88e3-48d5-8f98-49eb177fe8df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "df = df.dropDuplicates()\n",
    "df = df.filter(col(\"text\").isNotNull() & (col(\"text\") != \"\"))\n",
    "df = df.withColumn(\"text\", lower(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64dba55-a4a0-416c-bd81-41355b135fec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "df = remover.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074e1d4b-c17b-40d2-bd5b-1fdabf30bf5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save processed json file\n",
    "import os\n",
    "\n",
    "# extract filename from the original path\n",
    "filename = new_file.split(\"/\")[-1]\n",
    "\n",
    "# replace the .json extension with _processed.json\n",
    "processed_filename = filename.replace(\".json\", \"_processed.json\")\n",
    "\n",
    "# construct the new path for the processed data\n",
    "processed_path = new_file.rsplit(\"/\", 1)[0] + \"/\" + processed_filename\n",
    "\n",
    "# save the processed DataFrame to the new temp path\n",
    "temp_path = processed_path + \"_temp\"\n",
    "df.write.json(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa285dd2-dad6-484a-a47d-f9d9397578c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# locate the \"part-\" file in the temporary location\n",
    "files = dbutils.fs.ls(temp_path)\n",
    "part_file = next((f for f in files if f.name.startswith(\"part-\")), None)\n",
    "if part_file:\n",
    "    old_path = os.path.join(temp_path, part_file.name)\n",
    "\n",
    "    # rename this \"part-\" file to the desired location\n",
    "    new_path = os.path.join('/mnt/youtube-news-comments/youtube-comments/', processed_filename)\n",
    "    if dbutils.fs.cp(old_path, new_path):  \n",
    "        dbutils.fs.rm(temp_path, recurse=True)  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "youtube-comment-preprocessing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
